    <!-- 

⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣦⣶⣾⣿⣷⣶⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣶⣿⣟⠯⠓⣉⣩⣭⣝⣻⣿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣾⣿⠗⢡⣴⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⠏⣰⣿⣿⣿⣿⣿⣿⣿⠋⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⣿⣿⣿⢠⣿⣿⣿⣿⣿⠿⢿⣿⣀⣿⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣸⣿⣿⣿⠋⠀⠀⠀⣨⣩⠙⠀⢹⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⣿⣿⣿⣿⡏⣀⣀⣀⣀⢧⣿⠂⣀⠀⣿⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣶⣿⣿⠗⡤⢤⣀⡉⠊⡱⢋⣉⣉⣷⠄⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡝⣿⣿⠀⠈⠙⠿⠃⠀⡇⠽⠛⢻⡏⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣴⣿⣿⣿⣿⣿⡀⠀⣀⠤⠾⣄⡹⣄⠀⢸⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣋⢹⣿⣿⣷⡾⢄⠀⠀⠀⠀⢈⣶⣿⠿⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⣇⣸⣿⣿⣿⣿⣿⡏⢻⣿⣿⣇⠈⠡⢄⣀⠐⢉⣿⣿⣴⣿⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⣿⡿⠟⢻⣿⣿⣿⠀⠀⠻⣿⣿⣷⣤⣄⣠⣴⣿⣿⣿⣿⣿⣿⣇⡀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⣠⠔⠒⠒⠉⠀⠀⠀⣿⣿⣿⡇⠀⠀⠀⠉⠛⢿⣿⡿⠛⠋⠘⣿⣿⠿⢯⠛⠂⠤⢄⡀⠀⠀⠀⠀
⠀⣠⠊⠁⠀⠀⠀⠀⠀⠀⠀⢻⠁⠀⠸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠀⠀⠀⠀⠀⡇⠀
⣰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢇⠀⠀⢣⠒⠲⠤⣀⡀⠀⡀⣀⠤⠒⠂⠸⡀⠀⢱⠀⠀⠀⠀⠀⠀⠙⣄
⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢆⡀⠀⢣⡀⠀⠈⠀⠈⠀⠃⠀⠀⠀⠰⠧⠀⠚⠀⠀⠀⠀⠀⠀⠀⠙
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀jai Shree Ram 
                        💓💓⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
Vats Mera Source Code Chori Mat Karo , Khud ki website Banao!!! Samjhe ...
-->

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import requests
import base64
from io import BytesIO
import arxiv
import time
import re

# Page configuration
st.set_page_config(
    page_title="AI & ML powered Research Paper Finder",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .paper-card {
        background-color: #000000;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
    }
    .metric-card {
        background-color: #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
    }
    .download-btn {
        background-color: #28a745;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 5px;
        text-decoration: none;
        display: inline-block;
        margin-top: 0.5rem;
    }
    .stSelectbox div[data-baseweb="select"] > div {
        background-color: #f0f2f6 !important;
        color: #31333F !important;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_sample_data():
    """Create sample data for demonstration"""
    sample_data = {
        'title': [
            'Deep Reinforcement Learning for Autonomous Drone Navigation in Complex Environments',
            'Adaptive Control Systems for Multi-Rotor UAVs Using Machine Learning',
            'Swarm Intelligence and Collective Behavior in Drone Networks',
            'Neural Network-Based Path Planning for Quadcopter Systems',
            'Machine Learning Approaches to Drone Obstacle Avoidance'
        ],
        'summary': [
            'This paper presents a novel deep reinforcement learning approach for autonomous drone navigation in complex, dynamic environments. The proposed method demonstrates superior performance compared to traditional control systems.',
            'We introduce an adaptive control framework that utilizes machine learning techniques to optimize flight performance in varying environmental conditions. The system shows improved stability and efficiency.',
            'This study explores swarm intelligence algorithms for coordinating multiple drones in collaborative tasks. The adaptive behavior emerges from local interactions between individual agents.',
            'A comprehensive neural network architecture is proposed for real-time path planning in quadcopter systems. The method handles dynamic obstacles and uncertain environments effectively.',
            'This work presents multiple machine learning approaches for drone obstacle avoidance, comparing their effectiveness in different scenarios and environmental conditions.'
        ],
        'authors': [
            'Smith, J., Johnson, A., Williams, R.',
            'Brown, M., Davis, K., Miller, S.',
            'Wilson, T., Garcia, L., Anderson, P.',
            'Taylor, C., Thomas, D., Jackson, B.',
            'White, N., Harris, E., Martin, F.'
        ],
        'published_date': [
            '2023-12-15',
            '2023-11-20',
            '2023-10-05',
            '2023-09-18',
            '2023-08-30'
        ],
        'primary_category': [
            'cs.RO',
            'cs.LG',
            'cs.MA',
            'cs.AI',
            'cs.CV'
        ],
        'categories': [
            'cs.RO, cs.LG, cs.AI',
            'cs.LG, cs.RO, cs.SY',
            'cs.MA, cs.RO, cs.AI',
            'cs.AI, cs.RO, cs.LG',
            'cs.CV, cs.RO, cs.LG'
        ],
        'arxiv_id': [
            '2312.12345',
            '2311.11111',
            '2310.22222',
            '2309.33333',
            '2308.44444'
        ],
        'pdf_url': [
            'https://arxiv.org/pdf/2312.12345.pdf',
            'https://arxiv.org/pdf/2311.11111.pdf',
            'https://arxiv.org/pdf/2310.22222.pdf',
            'https://arxiv.org/pdf/2309.33333.pdf',
            'https://arxiv.org/pdf/2308.44444.pdf'
        ],
        'entry_url': [
            'https://arxiv.org/abs/2312.12345',
            'https://arxiv.org/abs/2311.11111',
            'https://arxiv.org/abs/2310.22222',
            'https://arxiv.org/abs/2309.33333',
            'https://arxiv.org/abs/2308.44444'
        ]
    }
    return pd.DataFrame(sample_data)

def clean_text(text: str) -> str:
    """Clean and normalize text content"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

@st.cache_data
def search_arxiv_papers(query: str, max_results: int = 200):
    """Search arXiv for papers and return structured data"""
    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )
    
    papers_data = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        results = list(client.results(search))
        total_results = len(results)
        
        if total_results == 0:
            st.warning("No papers found for the given query.")
            return pd.DataFrame()

        for i, result in enumerate(results):
            papers_data.append(get_paper_details(result))
            
            # Update progress
            progress = (i + 1) / total_results
            progress_bar.progress(progress)
            status_text.text(f"Processed {i + 1}/{total_results} papers...")
            
            time.sleep(0.1)  # Be respectful to arXiv API
            
    except Exception as e:
        st.error(f"Error during search: {e}")
        
    progress_bar.empty()
    status_text.empty()
    
    return pd.DataFrame(papers_data)

def get_paper_details(result):
    """Extract paper details from an arxiv search result."""
    return {
        'title': clean_text(result.title),
        'summary': clean_text(result.summary),
        'authors': ', '.join([author.name for author in result.authors]),
        'published_date': result.published.strftime('%Y-%m-%d'),
        'primary_category': result.primary_category,
        'categories': ', '.join(result.categories),
        'arxiv_id': result.entry_id.split('/')[-1],
        'pdf_url': result.pdf_url,
        'entry_url': result.entry_id
    }

def create_download_link(df, filename="arxiv_papers.csv"):
    """Create a download link for the dataset"""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}" class="download-btn">📥 Download Dataset (CSV)</a>'
    return href

def display_paper_card(paper, index):
    """Display a paper in a card format"""
    with st.container():
        st.markdown(f"""
        <div class="paper-card">
            <h3>{paper['title']}</h3>
            <p><strong>Authors:</strong> {paper['authors']}</p>
            <p><strong>Published:</strong> {paper['published_date']}</p>
            <p><strong>Categories:</strong> {paper['categories']}</p>
            <p><strong>Summary:</strong> {paper['summary'][:300]}...</p>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        with col1:
            st.link_button(
                "📄 View on arXiv", 
                paper['entry_url'],
                use_container_width=True
            )
        with col2:
            st.link_button(
                "📥 Download PDF", 
                paper['pdf_url'],
                use_container_width=True
            )

def main():
    # Header
    st.markdown('<h1 class="main-header">🧠 AI & ML powered Research Paper Finder</h1>', 
                unsafe_allow_html=True)
    
    # Sidebar
    st.sidebar.title("🔧 Controls")
    
    # Data source selection
    data_source = st.sidebar.radio(
        "Choose Data Source:",
        ["Load Sample Data", "Search arXiv Live"]
    )
    
    # Initialize session state
    if 'df' not in st.session_state:
        st.session_state.df = None
    
    # Data loading section
    if data_source == "Load Sample Data":
        if st.sidebar.button("Load Sample Dataset"):
            st.session_state.df = load_sample_data()
            st.sidebar.success("Sample data loaded!")
    
    else:  # Search arXiv Live
        st.sidebar.subheader("Search Parameters")
        
        search_terms = st.sidebar.text_input(
            "Custom Search Terms:", 
            "machine learning drone UAV adaptive"
        )
        
        max_results = st.sidebar.slider(
            "Maximum Results:", 
            min_value=10, 
            max_value=200, 
            value=50,
            step=10
        )
        
        if st.sidebar.button("🔍 Search arXiv"):
            query = f'({search_terms}) AND (cat:cs.RO OR cat:cs.LG OR cat:cs.AI OR cat:cs.CV OR cat:cs.MA OR cat:cs.SY)'
            
            with st.spinner("Searching arXiv database..."):
                st.session_state.df = search_arxiv_papers(query, max_results)
                
            if len(st.session_state.df) > 0:
                st.sidebar.success(f"Found {len(st.session_state.df)} papers!")
            else:
                st.sidebar.error("No papers found for the specified query.")
    
    # Display results if data is loaded
    if st.session_state.df is not None and len(st.session_state.df) > 0:
        df = st.session_state.df
        
        # Metrics dashboard
        st.subheader("📊 Dataset Overview")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="Total Papers",
                value=len(df)
            )
        
        with col2:
            unique_categories = len(df['primary_category'].unique())
            st.metric(
                label="Categories",
                value=unique_categories
            )
        
        with col3:
            if 'published_date' in df.columns:
                date_range = f"{df['published_date'].min()[:4]} - {df['published_date'].max()[:4]}"
            else:
                date_range = "N/A"
            st.metric(
                label="Date Range",
                value=date_range
            )
        
        with col4:
            avg_authors = df['authors'].str.count(',').mean() + 1 if 'authors' in df.columns else 0
            st.metric(
                label="Avg Authors",
                value=f"{avg_authors:.1f}"
            )
        
        # Download section
        st.subheader("💾 Download Dataset")
        filename = f"ml_drone_papers_{datetime.now().strftime('%Y%m%d')}.csv"
        st.markdown(create_download_link(df, filename), unsafe_allow_html=True)
        
        st.subheader(f"📋 View Raw Data")
        filtered_df = df.copy()
        st.dataframe(filtered_df, use_container_width=True)

        # Category distribution
        if 'primary_category' in df.columns:
            st.subheader("📈 Category Distribution")
            category_counts = df['primary_category'].value_counts()
            
            fig = px.bar(
                x=category_counts.index,
                y=category_counts.values,
                labels={'x': 'Category', 'y': 'Number of Papers'},
                title="Papers by Primary Category"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Publication timeline
        if 'published_date' in df.columns:
            st.subheader("📅 Publication Timeline")
            df['year'] = pd.to_datetime(df['published_date']).dt.year
            year_counts = df['year'].value_counts().sort_index()
            
            fig = px.line(
                x=year_counts.index,
                y=year_counts.values,
                labels={'x': 'Year', 'y': 'Number of Papers'},
                title="Publications by Year"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Search and filter
        st.subheader("🔍 Search Papers")
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_query = st.text_input("Search in titles and summaries:", "")
        
        with col2:
            if 'primary_category' in df.columns:
                category_filter = st.selectbox(
                    "Filter by category:",
                    ["All"] + list(df['primary_category'].unique())
                )
            else:
                category_filter = "All"
        
        # Filter dataframe
        
        
        if search_query:
            mask = (
                filtered_df['title'].str.contains(search_query, case=False, na=False) |
                filtered_df['summary'].str.contains(search_query, case=False, na=False)
            )
            filtered_df = filtered_df[mask]
        
        if category_filter != "All" and 'primary_category' in df.columns:
            filtered_df = filtered_df[filtered_df['primary_category'] == category_filter]
        
        # Display papers
        st.subheader(f"📚 Papers ({len(filtered_df)} found)")
        
        # Pagination
        papers_per_page = 5
        total_pages = (len(filtered_df) - 1) // papers_per_page + 1
        
        if total_pages > 1:
            page = st.selectbox("Page:", range(1, total_pages + 1))
        else:
            page = 1
        
        start_idx = (page - 1) * papers_per_page
        end_idx = start_idx + papers_per_page
        
        for i, (_, paper) in enumerate(filtered_df.iloc[start_idx:end_idx].iterrows()):
            display_paper_card(paper, start_idx + i)
        
        # # Raw data view
        # with st.expander("📋 View Raw Data"):
        #     st.dataframe(filtered_df, use_container_width=True)
    
    else:
        # Welcome message
        st.info("""
        👋 Welcome to the 🧠 AI & ML powered Research Paper Finder Database!
        
        **Getting Started:**
        1. Use the sidebar to either load sample data or search arXiv live
        2. Browse papers, filter by categories, or search for specific topics
        3. Click on paper titles to view full details on arXiv
        4. Download PDFs directly from the links provided
        5. Export the entire dataset as CSV for further analysis
        
        **Features:**
        - 📊 Interactive visualizations and statistics
        - 🔍 Advanced search and filtering
        - 📥 Direct PDF downloads
        - 💾 Dataset export functionality
        - 📱 Mobile-responsive design
        """)

if __name__ == "__main__":

    main()
